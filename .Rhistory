library(knitr)
library(doParallel)
library(dplyr)
library(checkmate)
library(gridExtra)
library(xtable)
library(reshape2)
library(data.table)
library(ggplot2)
library(cowplot)
library(h2o)
library(randomForest)
library(raster)
library(stringdist)
library(parallelMap)
library(parallel)
library(randomForest)
library(missForest)
library(ClusterR)
library(lubridate)
source("prepare.R")
pdfName <- "analysis_raw_data"
#get data
data <- read.csv2("bike-sharing-demand/train.csv", sep=",")
#predictData <- read.csv2("datadrivenproject/data/rawData/testSetValues.csv", sep=",")
data <- as.data.table(data)
colSums(is.na(data))
data <- prepareData(data)
#model----------------------------------
h2o.init()
autoModel <- h2o.automl(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data),
seed = 1)
library(knitr)
library(doParallel)
library(dplyr)
library(checkmate)
library(gridExtra)
library(xtable)
library(reshape2)
library(data.table)
library(ggplot2)
library(cowplot)
library(h2o)
library(randomForest)
library(raster)
library(stringdist)
library(parallelMap)
library(parallel)
library(randomForest)
library(missForest)
library(ClusterR)
library(lubridate)
source("prepare.R")
pdfName <- "analysis_raw_data"
#get data
data <- read.csv2("bike-sharing-demand/train.csv", sep=",")
#predictData <- read.csv2("datadrivenproject/data/rawData/testSetValues.csv", sep=",")
data <- as.data.table(data)
colSums(is.na(data))
data <- prepareData(data)
#model----------------------------------
h2o.init()
names(data)[-c(1,10,11)]
setwd("~/")
source("prepare.R")
setwd("~/R/bike")
setwd("~/R/bike")
source("prepare.R")
data <- read.csv2("bike-sharing-demand/train.csv", sep=",")
data <- as.data.table(data)
colSums(is.na(data))
data <- prepareData(data)
h2o.init()
data[,c(-1,-11)]
model <- h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5
)
model
seq(40, 70, 5)
hyper_params = list(learn_rate = seq(0.01, 0.1, 0.01),
max_depth = seq(2, 10, 1),
sample_rate = seq(0.5, 1.0, 0.1),
col_sample_rate = seq(0.1, 1.0, 0.1),
ntrees = seq(40, 70, 5)
)
search_criteria = list(strategy = "RandomDiscrete", max_runtime_secs = 1000, max_models = 50, stopping_metric = "AUTO", stopping_tolerance = 0.00001, stopping_rounds = 5, seed = 123476)
drf.grid <- h2o.grid("gbm",
grid_id = "mygrid",
x = names(data)[-c(1,10,11)],
y = "count",
# faster to use a 80/20 split
training_frame = as.h2o(data[,c(-1,-11)]),
#validation_frame = as.h2o(v),
nfolds = 5,
# alternatively, use N-fold cross-validation
#training_frame = train,
#nfolds = 5,
## stop as soon as mse doesn't improve by more than 0.1% on the validation set,
## for 2 consecutive scoring events
#stopping_rounds = 2,
#stopping_tolerance = 1e-3,
#stopping_metric = "MSE",
#score_tree_interval = 100, ## how often to score (affects early stopping)
seed = 123476,  ## seed to control the sampling of the Cartesian hyper-parameter space
hyper_params = hyper_params,
search_criteria = search_criteria)
drf.sorted.grid <- h2o.getGrid(grid_id = "mygrid1", sort_by = "r2")
drf.sorted.grid <- h2o.getGrid(grid_id = "mygrid", sort_by = "r2")
print(drf.sorted.grid)
hyper_params = list(learn_rate = seq(0.05, 0.2, 0.01),
max_depth = seq(8, 12, 1),
col_sample_rate = seq(0.8, 1.0, 0.01),
ntrees = seq(60, 74, 2)
)
search_criteria = list(strategy = "RandomDiscrete", max_runtime_secs = 1000, stopping_metric = "AUTO", stopping_tolerance = 0.00001, stopping_rounds = 5, seed = 123476)
h2o.shutdown()
h2o.init()
hyper_params = list(learn_rate = seq(0.05, 0.2, 0.01),
max_depth = seq(8, 12, 1),
col_sample_rate = seq(0.8, 1.0, 0.01),
ntrees = seq(60, 74, 2)
)
search_criteria = list(strategy = "RandomDiscrete", max_runtime_secs = 1000, stopping_metric = "AUTO", stopping_tolerance = 0.00001, stopping_rounds = 5, seed = 123476)
drf.grid <- h2o.grid("gbm",
grid_id = "mygrid",
x = names(data)[-c(1,10,11)],
y = "count",
# faster to use a 80/20 split
training_frame = as.h2o(data[,c(-1,-11)]),
#validation_frame = as.h2o(v),
nfolds = 5,
# alternatively, use N-fold cross-validation
#training_frame = train,
#nfolds = 5,
## stop as soon as mse doesn't improve by more than 0.1% on the validation set,
## for 2 consecutive scoring events
#stopping_rounds = 2,
#stopping_tolerance = 1e-3,
#stopping_metric = "MSE",
#score_tree_interval = 100, ## how often to score (affects early stopping)
seed = 123476,  ## seed to control the sampling of the Cartesian hyper-parameter space
hyper_params = hyper_params,
search_criteria = search_criteria)
drf.sorted.grid <- h2o.getGrid(grid_id = "mygrid", sort_by = "r2")
print(drf.sorted.grid)
model <- h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.84, learn_rate = 0.19,
max_depth = 8, ntrees = 66
)
model
model
model <- h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.84, learn_rate = 0.19,
max_depth = 8, ntrees = 66
)
model
model <- h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.84, learn_rate = 0.19,
max_depth = 8, ntrees = 66
)
model
h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.84, learn_rate = 0.12,
max_depth = 8, ntrees = 66
)
h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.84, learn_rate = 0.12,
max_depth = 8, ntrees = 66
)
print(drf.sorted.grid)
h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.82, learn_rate = 0.12,
max_depth = 9, ntrees = 74
)
h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.82, learn_rate = 0.12,
max_depth = 9, ntrees = 80
)
h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.82, learn_rate = 0.12,
max_depth = 9, ntrees = 80
)
h2o.gbm(x = names(data)[-c(1,10,11)], y = "count",
training_frame = as.h2o(data[,c(-1,-11)]),nfolds = 5,
col_sample_rate = 0.82, learn_rate = 0.12,
max_depth = 9, ntrees = 80
)
